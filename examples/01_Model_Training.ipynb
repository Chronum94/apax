{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this tutorial we are going to train a model from scratch on a molecular dataset from the MD17 collection.\n",
    "Start by creating a project folder and downloading the dataset.\n",
    "\n",
    "## Acquiring a dataset\n",
    "\n",
    "You can obtain the benzene dataset with DFT labels either by running the following command or manually from this [link](http://sgdml.org/?datasetID=benzene2018_dft). Apax uses ASE to read in datasets, so make sure to convert your own data into an ASE readable format (extxyz, traj etc). Be carefull the downloaded dataset has to be modified like in the `apax.untils.dataset.mod_md_datasets` function in order to be readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from apax.utils.datasets import download_benzene_DFT, mod_md_datasets, download_md22_benzene_CCSDT\n",
    "\n",
    "data_path = Path(\"project\")\n",
    "\n",
    "file_path = download_benzene_DFT(data_path)\n",
    "file_path = mod_md_datasets(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration files\n",
    "\n",
    "Next, we require a configuration file that specifies the model and training parameters.\n",
    "In order to get users quickly up and running, our command line interface provides an easy way to generate input templates.\n",
    "The provided templates come in in two levels of verbosity: minimal and full.\n",
    "In the following we are going to use a minimal input file. To see a complete list and explanation of all parameters, consult the documentation page LINK.\n",
    "For more information on the CLI,  simply run `apax -h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command create a minimal configuration file in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux3_i1/segreto/miniconda3/envs/apax/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a config file in the working directory.\n"
     ]
    }
   ],
   "source": [
    "!apax template train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the resulting `config.yaml` file in an editor of your choice and make sure to fill in the data path field with the name of the data set you just downloaded.\n",
    "For the purposes of this tutorial we will train on 1000 data points and validate the model on 200 more during the training. Further, the units of the labels have to be specified. Random splitting is done by apax but it is also possible to input a pre-splitted training and validation dataset\n",
    "\n",
    "The filled in configuration file should look similar to this one.\n",
    "\n",
    "```yaml\n",
    "epoch: 1000\n",
    "data:\n",
    "    data_path: md17.extexyz\n",
    "    epochs: 1000\n",
    "    n_train: 1000\n",
    "    energy_unit: kcal/mol\n",
    "    pos_unit: Ang\n",
    "    ....\n",
    "```\n",
    "\n",
    "It also can be modefied with the utils function `mod_config` provided by Apax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apax.utils.helpers import mod_config\n",
    "import yaml\n",
    "\n",
    "\n",
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 10,\n",
    "    \"data\": {\n",
    "        \"experiment\": \"benzene_dft_cli\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    }\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to check whether the a configuration file is valid, we provide the `validate` command. This is especially convenient when submitting training runs on a compute cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax validate train config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration files are validated using Pydantic and the errors provided by the `validate` command give precise instructions on how to fix the input file.\n",
    "For example, changing `epochs` to `-1000`, validate will give the following feedback to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_updates = {\n",
    "    \"n_epochs\": -1000,\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"error_config.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for Config\n",
      "n_epochs\n",
      "  Input should be greater than 0 [type=greater_than, input_value=-1000, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/greater_than\n",
      "\u001b[31mConfiguration Invalid!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apax validate train error_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Model training can be started by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax train config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "During training, apax displays a progress bar to keep track of the validation loss.\n",
    "This progress bar is optional however and can be turned off in the config. LINK\n",
    "The default configuration writes training metrics to a CSV file, but TensorBoard is also supported.\n",
    "One can specify which to use by adding the following section to the input file:\n",
    "\n",
    "```yaml\n",
    "callbacks:\n",
    "    - CSV\n",
    "```\n",
    "\n",
    "If training is interrupted for any reason, re-running the above `train` command will resume training from the latest checkpoint.\n",
    "\n",
    "Furthermore, an Apax trianing can easily be started within a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.units import kcal, mol, Ang\n",
    "from ase.io import read, write\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "atoms = read(file_path, \":\")\n",
    "for atom in atoms:\n",
    "    energy = atom.get_total_energy() * kcal/mol\n",
    "    forces = atom.get_forces() * kcal/mol/Ang\n",
    "    calc = SinglePointCalculator(atoms, energy=energy, forces=forces)\n",
    "    atom.calc = calc\n",
    "write(file_path, atoms)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing NL: 100%|███████████████████████████████████████| 1000/1000 [00:00<00:00, 12911.35it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m config_updates \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m mod_config(config_path, config_updates)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/dev/apax/apax/train/run.py:70\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(user_config, log_level)\u001b[0m\n\u001b[1;32m     68\u001b[0m train_raw_ds, val_raw_ds \u001b[38;5;241m=\u001b[39m load_data_files(config\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     69\u001b[0m train_ds, ds_stats \u001b[38;5;241m=\u001b[39m initialize_dataset(config, train_raw_ds)\n\u001b[0;32m---> 70\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_raw_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m train_ds\u001b[38;5;241m.\u001b[39mset_batch_size(config\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     73\u001b[0m val_ds\u001b[38;5;241m.\u001b[39mset_batch_size(config\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mvalid_batch_size)\n",
      "File \u001b[0;32m~/uni/dev/apax/apax/data/initialization.py:51\u001b[0m, in \u001b[0;36minitialize_dataset\u001b[0;34m(config, atoms_list, read_labels, calc_stats)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calc_stats \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m read_labels:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot calculate scale/shift parameters without reading labels.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43matoms_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_pbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_nl_pbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m labels \u001b[38;5;241m=\u001b[39m atoms_to_labels(\n\u001b[1;32m     58\u001b[0m     atoms_list,\n\u001b[1;32m     59\u001b[0m     additional_properties_info\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madditional_properties_info,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     energy_unit\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39menergy_unit,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calc_stats:\n",
      "File \u001b[0;32m~/uni/dev/apax/apax/data/input_pipeline.py:105\u001b[0m, in \u001b[0;36mprocess_inputs\u001b[0;34m(atoms_list, r_max, disable_pbar, pos_unit)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_inputs\u001b[39m(\n\u001b[1;32m    100\u001b[0m     atoms_list: \u001b[38;5;28mlist\u001b[39m,\n\u001b[1;32m    101\u001b[0m     r_max: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    102\u001b[0m     disable_pbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m     pos_unit: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAng\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43matoms_to_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43matoms_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_unit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     idx, offsets \u001b[38;5;241m=\u001b[39m dataset_neighborlist(\n\u001b[1;32m    107\u001b[0m         inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragged\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    108\u001b[0m         box\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m         disable_pbar\u001b[38;5;241m=\u001b[39mdisable_pbar,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragged\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m idx\n",
      "File \u001b[0;32m~/uni/dev/apax/apax/utils/convert.py:75\u001b[0m, in \u001b[0;36matoms_to_inputs\u001b[0;34m(atoms_list, pos_unit)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts an list of ASE atoms to a dict where all inputs\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mare sorted by their shape (ragged/fixed). Units are\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03madjusted if ASE compatible and provided in the inputpipeline.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Labels are trainable system properties.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragged\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     },\n\u001b[1;32m     73\u001b[0m }\n\u001b[0;32m---> 75\u001b[0m box \u001b[38;5;241m=\u001b[39m \u001b[43matoms_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39marray\n\u001b[1;32m     76\u001b[0m pbc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mall(box \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atoms \u001b[38;5;129;01min\u001b[39;00m atoms_list:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from apax.train.run import run\n",
    "\n",
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"experiment\": \"benzene_dft_script2\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    }\n",
    "}\n",
    "\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "run(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path = \"project/models/benzene_dft_script2/log.csv\"\n",
    "\n",
    "keys = [\"energy_mae\", \"forces_mse\", \"forces_mae\", \"loss\"]\n",
    "data_dict = {}\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    # Extract the headers (keys) from the first row\n",
    "    headers = next(reader)\n",
    "\n",
    "    # Initialize empty lists for each key\n",
    "    for header in headers:\n",
    "        data_dict[header] = []\n",
    "\n",
    "    # Read the rest of the rows and append values to the corresponding key\n",
    "    for row in reader:\n",
    "        for idx, value in enumerate(row):\n",
    "            key = headers[idx]\n",
    "            data_dict[key].append(float(value))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "fig.suptitle(f'Metrics', fontsize=16)\n",
    "\n",
    "for id, key in enumerate(keys):\n",
    "    val = np.array(data_dict[f\"val_{key}\"])\n",
    "    train = np.array(data_dict[f\"train_{key}\"])\n",
    "    epoch = np.array(data_dict[\"epoch\"])\n",
    "\n",
    "    axes[id].plot(epoch, val, label=\"val data\")\n",
    "    # axes[id].plot(epoch, train, label=\"train data\")\n",
    "\n",
    "    axes[id].set_ylabel(f\"{key}\")\n",
    "    axes[id].set_xlabel(r\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After the training is completed and we are satisfied with our choice of hyperparameters and vadliation loss, we can evaluate the model on the test set.\n",
    "We provide a separate command for test set evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apax.train.eval import eval_model\n",
    "\n",
    "eval_model(config_dict, n_test=100)\n",
    "# !apax eval config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO pretty print results to the terminal\n",
    "\n",
    "Congratulations, you have successfully trained and evaluated your first apax model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Closer Look At Training Parameters\n",
    "\n",
    "```yaml\n",
    "n_epochs: <NUMBER OF EPOCHS>  # Number of training epochs.\n",
    "seed: 1                       # Seed for initialising random numbers\n",
    "patience: None                # Number of epochs without improvement before trainings gets terminated.\n",
    "n_models: 1                   # Number of models to be trained at once.\n",
    "n_jitted_steps: 1             # Number of train batches to be processed in a compiled loop. \n",
    "                              # Can yield singificant speedups for small structures or small batch sizes.\n",
    "\n",
    "data:\n",
    "  directory: models/          # Path to the directory where the training results and checkpoints will be written.\n",
    "  experiment: apax            # Name of  the model. Distinguishes it from the other models trained in the same `directory`.\n",
    "  data_path: <PATH>           # Path to a single dataset file. Set either this or `val_data_path` and `train_data_path`.\n",
    "  train_data_path: <PATH>     # Path to a training dataset. Set this and `val_data_path` if your data comes pre-split.\n",
    "  val_data_path: <PATH>       # Path to a validation dataset. Set this and `train_data_path` if your data comes pre-split.\n",
    "  test_data_path: <PATH>      # Path to a test dataset. Set this, `train_data_path` and `val_data_path` if your data comes pre-split.\n",
    "\n",
    "  n_train: 1000               # Number of training datapoints from `data_path`.\n",
    "  n_valid: 100                # Number of validation datapoints from `data_path`.\n",
    "\n",
    "  batch_size: 32              # Number of training examples to be evaluated at once.\n",
    "  valid_batch_size: 100       # Number of validation examples to be evaluated at once.\n",
    "\n",
    "  shift_method: \"per_element_regression_shift\"\n",
    "  shift_options:\n",
    "    energy_regularisation: 1.0    # Magnitude of the regularization in the per-element energy regression.\n",
    "  shuffle_buffer_size: 1000       # Size of the `tf.data` shuffle buffer.\n",
    "\n",
    "  pos_unit: Ang\n",
    "  energy_unit: eV\n",
    "\n",
    "  additional_properties_info:     # Dict of property name, shape (ragged or fixed) pairs\n",
    "\n",
    "model:\n",
    "  n_basis: 7                  # Number of uncontracted gaussian basis functions.\n",
    "  n_radial: 5                 # Number of contracted basis functions.\n",
    "  nn: [512, 512]              # Number of hidden layers and units in those layers.\n",
    "\n",
    "  r_max: 6.0                  # Position of the first uncontracted basis function's mean.\n",
    "  r_min: 0.5                  # Cutoff radius of the descriptor.\n",
    "\n",
    "  use_zbl: false              # \n",
    "\n",
    "  b_init: normal              # Initialization scheme for the neural network biases. Either `normal` or `zeros`.\n",
    "  descriptor_dtype: fp64\n",
    "  readout_dtype: fp32\n",
    "  scale_shift_dtype: fp32\n",
    "\n",
    "loss:\n",
    "- loss_type: structures       # Weighting scheme for atomic contributions.\n",
    "                              # See the MLIP package for reference 10.1088/2632-2153/abc9fe for details\n",
    "  name: energy                # Keyword of the quantity e.g `energy`.\n",
    "  weight: 1.0                 # Weighting factor in the overall loss function.\n",
    "- loss_type: structures\n",
    "  name: forces\n",
    "  weight: 4.0\n",
    "\n",
    "metrics:\n",
    "- name: energy                # Keyword of the quantity e.g `energy`.\n",
    "  reductions:                 # List of reductions performed on the difference between target and predictions.\n",
    "                              # Can be mae, mse, rmse for energies and forces. For forces it is also possible to use `angle`.\n",
    "  - mae\n",
    "- name: forces\n",
    "  reductions:\n",
    "  - mae\n",
    "  - mse\n",
    "\n",
    "optimizer:\n",
    "  opt_name: adam            # Name of the optimizer. Can be any `optax` optimizer.\n",
    "  opt_kwargs: {}            # Optimizer keyword arguments. Passed to the `optax` optimizer.\n",
    "  emb_lr: 0.03              # Learning rate of the elemental embedding contraction coefficients.\n",
    "  nn_lr: 0.03               # Learning rate of the neural network parameters.\n",
    "  scale_lr: 0.001           # Learning rate of the elemental output scaling factors.\n",
    "  shift_lr: 0.05            # Learning rate of the elemental output shifts.\n",
    "  zbl_lr: 0.001             # \n",
    "  transition_begin: 0       # Number of training steps (not epochs) before the start of the linear learning rate schedule.\n",
    "\n",
    "callbacks:\n",
    "- name: csv                 # Keyword of the callback used. Currently we implement \"csv\" and \"tensorboard\".\n",
    "\n",
    "progress_bar:\n",
    "  disable_epoch_pbar: false   # Set to True to disable the epoch progress bar.\n",
    "  disable_nl_pbar: false      # Set to True to disable the NL precomputation progress bar.\n",
    "\n",
    "\n",
    "checkpoints:\n",
    "  ckpt_interval: 1                # Number of epochs between checkpoints.\n",
    "  \n",
    "                                  # The options below are used for transfer learning\n",
    "  base_model_checkpoint: null     # Path to the folder containing a pre-trained model ckpt.\n",
    "  reset_layers: []                # List of layer names for which the parameters will be reinitialized.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter                  | Default Value                  | Description                                                                 |\n",
    "|----------------------------|--------------------------------|-----------------------------------------------------------------------------|\n",
    "| **n_epochs**               | `<NUMBER OF EPOCHS>`           | Number of training epochs.                                                  |\n",
    "| **seed**                   | 1                              | Seed for initializing random numbers.                                       |\n",
    "| **patience**               | None                           | Number of epochs without improvement before training termination.          |\n",
    "| **n_models**               | 1                              | Number of models trained simultaneously.                                    |\n",
    "| **n_jitted_steps**         | 1                              | Number of train batches in a compiled loop. Can speed up for small batches. |\n",
    "| **Data**                   |                                |                                                                             |\n",
    "| directory                  | models/                        | Path to directory where training results and checkpoints are written.      |\n",
    "| experiment                 | apax                           | Model name distinguishing from others in directory.                         |\n",
    "| data_path                  | `<PATH>`                       | Path to single dataset file.                                               |\n",
    "| train_data_path            | `<PATH>`                       | Path to training dataset.                                                   |\n",
    "| val_data_path              | `<PATH>`                       | Path to validation dataset.                                                 |\n",
    "| test_data_path             | `<PATH>`                       | Path to test dataset.                                                       |\n",
    "| n_train                    | 1000                           | Number of training data points.                                             |\n",
    "| n_valid                    | 100                            | Number of validation data points.                                           |\n",
    "| batch_size                 | 32                             | Number of training examples evaluated at once.                             |\n",
    "| valid_batch_size           | 100                            | Number of validation examples evaluated at once.                           |\n",
    "| shift_method               | \"per_element_regression_shift\" | Method for shifting.                                                        |\n",
    "| shift_options              | energy_regularization: 1.0     | Regularization magnitude for energy regression.                             |\n",
    "| shuffle_buffer_size        | 1000                           | Size of `tf.data` shuffle buffer.                                           |\n",
    "| pos_unit                   | Ang                            | Positional unit.                                                            |\n",
    "| energy_unit                | eV                             | Energy unit.                                                                |\n",
    "| additional_properties_info |                                | Dictionary of property name, shape pairs.                                   |\n",
    "| **Model**                  |                                |                                                                             |\n",
    "| n_basis                    | 7                              | Number of Gaussian basis functions.                                         |\n",
    "| n_radial                   | 5                              | Number of contracted basis functions.                                      |\n",
    "| nn                         | [512, 512]                     | Hidden layers and units.                                                    |\n",
    "| r_max                      | 6.0                            | Maximum position of first basis function's mean.                           |\n",
    "| r_min                      | 0.5                            | Descriptor cutoff radius.                                                   |\n",
    "| use_zbl                    | false                          | Use Zero-Body-Loss.                                                         |\n",
    "| b_init                     | normal                         | Initialization scheme for biases.                                           |\n",
    "| descriptor_dtype           | fp64                           | Descriptor data type.                                                       |\n",
    "| readout_dtype              | fp32                           | Readout data type.                                                          |\n",
    "| scale_shift_dtype          | fp32                           | Scale/Shift data type.                                                      |\n",
    "| **Loss**                   |                                |                                                                             |\n",
    "| loss_type                  | structures                     | Weighting scheme for atomic contributions.                                 |\n",
    "| name                       | energy                         | Quantity keyword.                                                           |\n",
    "| weight                     | 1.0                            | Weighting factor in loss function.                                          |\n",
    "| name                       | forces                         | Quantity keyword.                                                           |\n",
    "| weight                     | 4.0                            | Weighting factor in loss function.                                          |\n",
    "| **Metrics**                |                                |                                                                             |\n",
    "| name                       | energy                         | Quantity keyword.                                                           |\n",
    "| reductions                 |                                | List of reductions on target-prediction differences.                        |\n",
    "| name                       | forces                         | Quantity keyword.                                                           |\n",
    "| reductions                 | mae, mse                       | Reductions on target-prediction differences.                               |\n",
    "| **Optimizer**              |                                |                                                                             |\n",
    "| opt_name                   | adam                           | Optimizer name.                                                             |\n",
    "| opt_kwargs                 | {}                             | Optimizer keyword arguments.                                                |\n",
    "| emb_lr                     | 0.03                           | Learning rate for elemental embedding contraction coefficients.            |\n",
    "| nn_lr                      | 0.03                           | Learning rate for neural network parameters.                                |\n",
    "| scale_lr                   | 0.001                          | Learning rate for elemental output scaling factors.                        |\n",
    "| shift_lr                   | 0.05                           | Learning rate for elemental output shifts.                                  |\n",
    "| zbl_lr                     | 0.001                          | Learning rate for Zero-Body-Loss.                                           |\n",
    "| transition_begin           | 0                              | Training steps before linear learning rate schedule.                        |\n",
    "| **Callbacks**              |                                |                                                                             |\n",
    "| name                       | csv                            | Callback name.                                                              |\n",
    "| **Progress Bar**           |                                |                                                                             |\n",
    "| disable_epoch_pbar         | false                          | Disable epoch progress bar.                                                 |\n",
    "| disable_nl_pbar            | false                          | Disable NL precomputation progress bar.                                     |\n",
    "| **Checkpoints**            |                                |                                                                             |\n",
    "| ckpt_interval              | 1                              | Epochs between checkpoints.                                                 |\n",
    "| base_model_checkpoint     | null                           | Path to pre-trained model checkpoint.                                       |\n",
    "| reset_layers               | []                             | List of layers to reinitialize parameters.                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove all the created files and clean up yor working directory run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'eval.log': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r project config.yaml error_config.yaml eval.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apax311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
