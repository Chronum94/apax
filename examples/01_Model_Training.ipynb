{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this tutorial we are going to train a model from scratch on a molecular dataset from the MD17 collection.\n",
    "Start by creating a project folder and downloading the dataset.\n",
    "\n",
    "## Acquiring a dataset\n",
    "\n",
    "You can obtain the benzene dataset with DFT labels either by running the following command or manually from this [link](http://www.quantum-machine.org/gdml/data/xyz/benzene2018_dft.zip). Apax uses ASE to read in datasets, so make sure to convert your own data into an ASE readable format (extxyz, traj etc). Be carefull the downloaded dataset has to be modified like in the `apax.untils.dataset.mop_md17` function in order to be readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from apax.utils.datasets import download_md17_benzene_DFT, mod_md17\n",
    "\n",
    "data_path = Path(\"project\")\n",
    "\n",
    "file_path = download_md17_benzene_DFT(data_path)\n",
    "file_path = mod_md17(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration files\n",
    "\n",
    "Next, we require a configuration file that specifies the model and training parameters.\n",
    "In order to get users quickly up and running, our command line interface provides an easy way to generate input templates.\n",
    "The provided templates come in in two levels of verbosity: minimal and full.\n",
    "In the following we are going to use a minimal input file. To see a complete list and explanation of all parameters, consult the documentation page LINK.\n",
    "For more information on the CLI,  simply run `apax -h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command create a minimal configuration file in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax template train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the resulting `config.yaml` file in an editor of your choice and make sure to fill in the data path field with the name of the data set you just downloaded.\n",
    "For the purposes of this tutorial we will train on 1000 data points and validate the model on 200 more during the training. Further, the units of the labels have to be specified. Random splitting is done by apax but it is also possible to input a pre-splitted training and validation dataset\n",
    "\n",
    "The filled in configuration file should look similar to this one.\n",
    "\n",
    "```yaml\n",
    "epoch: 1000\n",
    "data:\n",
    "    data_path: md17.extexyz\n",
    "    epochs: 1000\n",
    "    n_train: 1000\n",
    "    energy_unit: kcal/mol\n",
    "    pos_unit: Ang\n",
    "    ....\n",
    "```\n",
    "\n",
    "It also can be modefied with the utils function `mod_config` provided by Apax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apax.utils.helpers import mod_config\n",
    "import yaml\n",
    "\n",
    "\n",
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 10,\n",
    "    \"data\": {\n",
    "        \"experiment\": \"benzene_dft_cli\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    }\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to check whether the a configuration file is valid, we provide the `validate` command. This is especially convenient when submitting training runs on a compute cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax validate train config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration files are validated using Pydantic and the errors provided by the `validate` command give precise instructions on how to fix the input file.\n",
    "For example, changing `epochs` to `-1000`, validate will give the following feedback to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_updates = {\n",
    "    \"n_epochs\": -1000,\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"error_config.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax validate train error_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Model training can be started by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax train config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "During training, apax displays a progress bar to keep track of the validation loss.\n",
    "This progress bar is optional however and can be turned off in the config. LINK\n",
    "The default configuration writes training metrics to a CSV file, but TensorBoard is also supported.\n",
    "One can specify which to use by adding the following section to the input file:\n",
    "\n",
    "```yaml\n",
    "callbacks:\n",
    "    - CSV\n",
    "```\n",
    "\n",
    "If training is interrupted for any reason, re-running the above `train` command will resume training from the latest checkpoint.\n",
    "\n",
    "Furthermore, an Apax trianing can easily be started within a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apax.train.run import run\n",
    "\n",
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"experiment\": \"benzene_dft_script\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    }\n",
    "}\n",
    "\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "run(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path = \"project/models/benzene_dft_script/log.csv\"\n",
    "\n",
    "keys = [\"energy_mae\", \"forces_mse\", \"forces_mae\", \"loss\"]\n",
    "data_dict = {}\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    # Extract the headers (keys) from the first row\n",
    "    headers = next(reader)\n",
    "\n",
    "    # Initialize empty lists for each key\n",
    "    for header in headers:\n",
    "        data_dict[header] = []\n",
    "\n",
    "    # Read the rest of the rows and append values to the corresponding key\n",
    "    for row in reader:\n",
    "        for idx, value in enumerate(row):\n",
    "            key = headers[idx]\n",
    "            data_dict[key].append(float(value))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "fig.suptitle(f'Metrics', fontsize=16)\n",
    "\n",
    "for id, key in enumerate(keys):\n",
    "    val = np.array(data_dict[f\"val_{key}\"])\n",
    "    train = np.array(data_dict[f\"train_{key}\"])\n",
    "    epoch = np.array(data_dict[\"epoch\"])\n",
    "\n",
    "    axes[id].plot(epoch, val, label=\"val data\")\n",
    "    axes[id].plot(epoch, train, label=\"train data\")\n",
    "\n",
    "    axes[id].set_ylabel(f\"{key}\")\n",
    "    axes[id].set_xlabel(r\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After the training is completed and we are satisfied with our choice of hyperparameters and vadliation loss, we can evaluate the model on the test set.\n",
    "We provide a separate command for test set evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apax.train.eval import eval_model\n",
    "\n",
    "eval_model(config_dict, n_test=100)\n",
    "# !apax eval config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO pretty print results to the terminal\n",
    "\n",
    "Congratulations, you have successfully trained and evaluated your first apax model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Closer Look At Training Parameters\n",
    "\n",
    "```yaml\n",
    "n_epochs: <NUMBER OF EPOCHS> # Number of training epochs.\n",
    "seed: 1 # Seed for initialising random numbers\n",
    "patience: None # Number of epochs without improvement before trainings gets terminated.\n",
    "n_models: 1 # Number of models to be trained at once.\n",
    "n_jitted_steps: 1 # Number of train batches to be processed in a compiled loop. Can yield singificant speedups for small structures or small batch sizes.\n",
    "\n",
    "data:\n",
    "  directory: models/ # Path to the directory where the training results and checkpoints will be written.\n",
    "  experiment: apax # Name of  the model. Distinguishes it from the other models trained in the same `directory`.\n",
    "  data_path: <PATH> # Path to a single dataset file. Set either this or `val_data_path` and `train_data_path`.\n",
    "  train_data_path: <PATH> # Path to a training dataset. Set this and `val_data_path` if your data comes pre-split.\n",
    "  val_data_path: <PATH> # Path to a validation dataset. Set this and `train_data_path` if your data comes pre-split.\n",
    "  test_data_path: <PATH> # Path to a test dataset. Set this, `train_data_path` and `val_data_path` if your data comes pre-split.\n",
    "\n",
    "  n_train: 1000 # Number of training datapoints from `data_path`.\n",
    "  n_valid: 100 # Number of validation datapoints from `data_path`.\n",
    "\n",
    "  batch_size: 32 # Number of training examples to be evaluated at once.\n",
    "  valid_batch_size: 100 # Number of validation examples to be evaluated at once.\n",
    "\n",
    "  shift_method: \"per_element_regression_shift\"\n",
    "  shift_options:\n",
    "    energy_regularisation: 1.0 # Magnitude of the regularization in the per-element energy regression.\n",
    "  shuffle_buffer_size: 1000 # Size of the `tf.data` shuffle buffer.\n",
    "\n",
    "  pos_unit: Ang\n",
    "  energy_unit: eV\n",
    "\n",
    "  additional_properties_info: # Dict of property name, shape (ragged or fixed) pairs\n",
    "\n",
    "model:\n",
    "  n_basis: 7 # Number of uncontracted gaussian basis functions.\n",
    "  n_radial: 5 # Number of contracted basis functions.\n",
    "  nn: [512, 512] # Number of hidden layers and units in those layers.\n",
    "\n",
    "  r_max: 6.0 # Position of the first uncontracted basis function's mean.\n",
    "  r_min: 0.5 # Cutoff radius of the descriptor.\n",
    "\n",
    "  use_zbl: false # \n",
    "\n",
    "  b_init: normal # Initialization scheme for the neural network biases. Either `normal` or `zeros`.\n",
    "  descriptor_dtype: fp64\n",
    "  readout_dtype: fp32\n",
    "  scale_shift_dtype: fp32\n",
    "\n",
    "loss:\n",
    "- loss_type: structures # Weighting scheme for atomic contributions. See the MLIP package for reference 10.1088/2632-2153/abc9fe for details\n",
    "  name: energy # Keyword of the quantity e.g `energy`.\n",
    "  weight: 1.0 # Weighting factor in the overall loss function.\n",
    "- loss_type: structures\n",
    "  name: forces\n",
    "  weight: 4.0\n",
    "\n",
    "metrics:\n",
    "- name: energy # Keyword of the quantity e.g `energy`.\n",
    "  reductions: # List of reductions performed on the difference between target and predictions. Can be mae, mse, rmse for energies and forces. For forces it is also possible to use `angle`.\n",
    "  - mae\n",
    "- name: forces\n",
    "  reductions:\n",
    "  - mae\n",
    "  - mse\n",
    "\n",
    "optimizer:\n",
    "  opt_name: adam # Name of the optimizer. Can be any `optax` optimizer.\n",
    "  opt_kwargs: {} # Optimizer keyword arguments. Passed to the `optax` optimizer.\n",
    "  emb_lr: 0.03 # Learning rate of the elemental embedding contraction coefficients.\n",
    "  nn_lr: 0.03 # Learning rate of the neural network parameters.\n",
    "  scale_lr: 0.001 # Learning rate of the elemental output scaling factors.\n",
    "  shift_lr: 0.05 # Learning rate of the elemental output shifts.\n",
    "  zbl_lr: 0.001 # \n",
    "  transition_begin: 0 # Number of training steps (not epochs) before the start of the linear learning rate schedule.\n",
    "\n",
    "callbacks:\n",
    "- name: csv # Keyword of the callback used. Currently we implement \"csv\" and \"tensorboard\".\n",
    "\n",
    "progress_bar:\n",
    "  disable_epoch_pbar: false # Set to True to disable the epoch progress bar.\n",
    "  disable_nl_pbar: false # Set to True to disable the NL precomputation progress bar.\n",
    "\n",
    "\n",
    "checkpoints:\n",
    "  ckpt_interval: 1 # Number of epochs between checkpoints.\n",
    "  # The options below are used for transfer learning\n",
    "  base_model_checkpoint: null # Path to the folder containing a pre-trained model ckpt.\n",
    "  reset_layers: [] # List of layer names for which the parameters will be reinitialized.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove all the created files and clean up yor working directory run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r project config.yaml error_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apax311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
