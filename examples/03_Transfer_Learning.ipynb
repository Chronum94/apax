{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Datasets computed at high levels of theory are expensive and thus, usually small. \n",
    "A model trained on this data might not be able to generalize well to unseen configurations.\n",
    "Sometimes this can be remedied with transfer learning:\n",
    "By first training a model on a lot of data from a less expensive level of theory, only small adjustments to the parameters are required to accurately reproduce the potential energy surface of a different level of theory.\n",
    "\n",
    "\n",
    "Alternatively, the level of theory might not change, but the dataset is extended.\n",
    "This is the case in learning on the fly scenarios.\n",
    "For a demonstration of using transfer learning for learning on the fly, see the corresponding example from the [IPSuite documentation](https://ipsuite.readthedocs.io/en/latest/).\n",
    "\n",
    "\n",
    "Apax comes with discriminative transfer learning capabilities out of the box.\n",
    "In this tutorial we are going to fine tune a model trained on benzene data at the DFT level of theory to CCSDT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First download the appropriate dataset from the sgdml website.\n",
    "\n",
    "\n",
    "Transfer learning can be facilitated in apax by adding the path to a pre-trained model in the config.\n",
    "Furthermore, we can freeze or reduce the learning rate of various components by adjusting the `optimizer` section of the config.\n",
    "\n",
    "```yaml\n",
    "optimizer:\n",
    "    nn_lr: 0.004\n",
    "    embedding_lr: 0.0\n",
    "```\n",
    "\n",
    "Learning rates of 0.0 will mask the respective weights during training steps.\n",
    "Here, we will freeze the descriptor, reinitialize the scaling and shifting parameters and reduce the learning rate of all other components.\n",
    "\n",
    "We can now fine tune the model by running\n",
    "`apax train config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "from apax.utils.datasets import (\n",
    "    download_benzene_DFT,\n",
    "    download_md22_benzene_CCSDT,\n",
    "    mod_md_datasets,\n",
    ")\n",
    "from apax.utils.helpers import mod_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Datasets\n",
    "\n",
    "For this demonstration we will use the DFT and CC versions of the benzene MD17 dataset.\n",
    "We start by downloading both and saving them in an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DFT Data\n",
    "\n",
    "data_path = Path(\"project\")\n",
    "dft_file_path = download_benzene_DFT(data_path)\n",
    "dft_file_path = mod_md_datasets(dft_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CCSD(T) Data\n",
    "\n",
    "data_path = Path(\"project\")\n",
    "cc_file_path, _ = download_md22_benzene_CCSDT(data_path)\n",
    "cc_file_path = mod_md_datasets(cc_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Model\n",
    "\n",
    "First, we will train a model on the \"large\" (in relative terms) but less accurate DFT dataset.\n",
    "A standard model with default optimizers will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apax template train --full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"config_full.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"n_train\": 1000,\n",
    "        \"n_valid\": 200,\n",
    "        \"batch_size\": 8,\n",
    "        \"valid_batch_size\": 100,\n",
    "        \"experiment\": \"benzene_dft\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(dft_file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    },\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config_full.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732124930.427330  459463 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732124930.430484  459463 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO | 17:48:53 | Running on [CudaDevice(id=0)]\n",
      "INFO | 17:48:53 | Initializing Callbacks\n",
      "INFO | 17:48:53 | Initializing Loss Function\n",
      "INFO | 17:48:53 | Initializing Metrics\n",
      "INFO | 17:48:53 | Running Input Pipeline\n",
      "INFO | 17:48:53 | Reading data file project/benzene_mod.xyz\n",
      "INFO | 17:49:00 | Found n_train: 1000, n_val: 200\n",
      "INFO | 17:49:00 | Computing per element energy regression.\n",
      "INFO | 17:49:01 | Building Standard model\n",
      "INFO | 17:49:01 | initializing 1 model(s)\n",
      "INFO | 17:49:08 | Initializing Optimizer\n",
      "INFO | 17:49:08 | Beginning Training\n",
      "Epochs:   0%|                                                               | 0/100 [00:00<?, ?it/s]WARNING | 17:49:16 | SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.\n",
      "Epochs: 100%|████████████████████████████████████| 100/100 [00:52<00:00,  1.89it/s, val_loss=0.0206]\n",
      "INFO | 17:50:01 | Finished training\n"
     ]
    }
   ],
   "source": [
    "!apax train config_full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CC Training\n",
    "\n",
    "Next, we require a CC baseline to quantify the effect of pretraining.\n",
    "As with the DFT dataset, we will only use a small fraction of the data to emphasize the effects in the low-data regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"config_full.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"n_train\": 50,\n",
    "        \"n_valid\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"valid_batch_size\": 10,\n",
    "        \"experiment\": \"benzene_cc_baseline\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(cc_file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    },\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config_cc_baseline.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732125003.227050  460393 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732125003.230272  460393 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO | 17:50:05 | Running on [CudaDevice(id=0)]\n",
      "INFO | 17:50:05 | Initializing Callbacks\n",
      "INFO | 17:50:05 | Initializing Loss Function\n",
      "INFO | 17:50:05 | Initializing Metrics\n",
      "INFO | 17:50:05 | Running Input Pipeline\n",
      "INFO | 17:50:05 | Reading data file project/benzene_ccsd_t-train_mod.xyz\n",
      "INFO | 17:50:05 | Found n_train: 50, n_val: 10\n",
      "INFO | 17:50:05 | Computing per element energy regression.\n",
      "INFO | 17:50:06 | Building Standard model\n",
      "INFO | 17:50:06 | initializing 1 model(s)\n",
      "INFO | 17:50:13 | Initializing Optimizer\n",
      "INFO | 17:50:13 | Beginning Training\n",
      "Epochs:   0%|                                                               | 0/100 [00:00<?, ?it/s]WARNING | 17:50:20 | SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.\n",
      "Epochs: 100%|██████████████████████████████████████| 100/100 [00:29<00:00,  3.39it/s, val_loss=0.12]\n",
      "INFO | 17:50:42 | Finished training\n"
     ]
    }
   ],
   "source": [
    "!apax train config_cc_baseline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFT -> CC Fine Tuning\n",
    "\n",
    "Finally, we can fine tune a model that was pretrained on DFT data.\n",
    "The model architecture remains unchanged for all 3 runs.\n",
    "However, for fine-tuning we need to specify the path to the base model and how to deal with its parameters.\n",
    "For each parameter group we can choose to freeze, to reset it or to keep training it.\n",
    "It is certainly advisable to experiment with different strategies, but a good start consists in freezing the embedding layer if the system we transfer to remains the same and resetting the scale-shift layer if the level of theory changes (DFT and CC have different energy scales).\n",
    "\n",
    "Make sure to carefully inspect the config options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"config_full.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"n_train\": 50,\n",
    "        \"n_valid\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"valid_batch_size\": 10,\n",
    "        \"experiment\": \"benzene_cc_ft\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(cc_file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"emb_lr\": 0.00,  # freeze embedding layer\n",
    "        \"nn_lr\": 0.0005,  # lower lr\n",
    "        \"scale_lr\": 0.001,  # lower lr\n",
    "        \"shift_lr\": 0.005,  # lower lr\n",
    "    },\n",
    "    \"checkpoints\": {\n",
    "        \"base_model_checkpoint\": \"project/models/benzene_dft\",  # pretrained model\n",
    "        \"reset_layers\": [\"scale_shift\"],  # reset scale-shift layer\n",
    "    },\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config_cc_ft.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732125044.359099  461322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732125044.362317  461322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO | 17:50:46 | Running on [CudaDevice(id=0)]\n",
      "INFO | 17:50:46 | Initializing Callbacks\n",
      "INFO | 17:50:46 | Initializing Loss Function\n",
      "INFO | 17:50:46 | Initializing Metrics\n",
      "INFO | 17:50:46 | Running Input Pipeline\n",
      "INFO | 17:50:46 | Reading data file project/benzene_ccsd_t-train_mod.xyz\n",
      "INFO | 17:50:46 | Found n_train: 50, n_val: 10\n",
      "INFO | 17:50:46 | Computing per element energy regression.\n",
      "INFO | 17:50:46 | Building Standard model\n",
      "INFO | 17:50:46 | initializing 1 model(s)\n",
      "INFO | 17:50:53 | Initializing Optimizer\n",
      "INFO | 17:50:53 | loading checkpoint from project/models/benzene_dft/best\n",
      "INFO | 17:50:53 | Transferring parameters from project/models/benzene_dft\n",
      "INFO | 17:50:53 | Transferring parameter: radial_fn\n",
      "INFO | 17:50:53 | Transferring parameter: dense_0\n",
      "INFO | 17:50:53 | Transferring parameter: dense_0\n",
      "INFO | 17:50:53 | Transferring parameter: dense_1\n",
      "INFO | 17:50:53 | Transferring parameter: dense_1\n",
      "INFO | 17:50:53 | Transferring parameter: dense_2\n",
      "INFO | 17:50:53 | Transferring parameter: dense_2\n",
      "INFO | 17:50:53 | Beginning Training\n",
      "Epochs:   0%|                                                               | 0/100 [00:00<?, ?it/s]WARNING | 17:50:59 | SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.\n",
      "Epochs: 100%|████████████████████████████████████| 100/100 [00:25<00:00,  3.91it/s, val_loss=0.0078]\n",
      "INFO | 17:51:19 | Finished training\n"
     ]
    }
   ],
   "source": [
    "!apax train config_cc_ft.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the fine-tuned model achieves a lower validation loss than the baseline CC model.\n",
    "\n",
    "How much further can you improve the fine-tuning (or pretraining) setup?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_defaults",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
